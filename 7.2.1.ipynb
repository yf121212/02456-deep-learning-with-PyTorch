{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "“7.2-EXE-variational-autoencoder.ipynb”的副本",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yf121212/02456-deep-learning-with-PyTorch/blob/master/7.2.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_8lQoMs5YIw"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "This is an exercise to be handed in on Peergrade\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iK_7tSU5YIx"
      },
      "source": [
        "from typing import *\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display, clear_output\n",
        "import numpy as np\n",
        "%matplotlib nbagg\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "try:\n",
        "    from plotting import make_vae_plots\n",
        "except Exception as ex:\n",
        "    print(f\"If using Colab, you may need to upload `plotting.py`. \\\n",
        "          \\nIn the left pannel, click `Files > upload to session storage` and select the file `plotting.py` from your computer \\\n",
        "          \\n---------------------------------------------\")\n",
        "    print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es6Z0Sz25YI0"
      },
      "source": [
        "*You should first read the introduction from the Notebook about Autoencoders (notebook 7.1).*\n",
        "\n",
        "As background material we recommend reading [Tutorial on Variational Autoencoder](http://arxiv.org/abs/1606.05908). For the implementation of the model you can read the article \"Auto-Encoding Variational Bayes\", Kingma & Welling, ICLR 2014: http://arxiv.org/pdf/1312.6114v10.pdf and \"Stochastic Backpropagation and Approximate Inference in Deep Generative Models\", Rezende et al, ICML 2014: http://arxiv.org/pdf/1401.4082v3.pdf.\n",
        "\n",
        "# Variational Autoencoders\n",
        "\n",
        "Probabilistic Machine Learning is fundamental in modern machine learning. While deep learning methods have been criticized for their lack of explainability, building machine learning models using probabilistic principles provides statistical guarantees. This is a key component for deploying machine learning to critical infrastructures (healthcare, manufacturing, finance, etc..). The Variational Autoencoder embodies probabilistic principles for principled unsupervised learning.\n",
        "\n",
        "## 1. Probabilistic Design:  Designing a Generative Process\n",
        "\n",
        "<img src=\"https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/7_Unsupervised/static/vae.png?raw=1\" />\n",
        "\n",
        "In the notebook 7.1, the goal was to learn a set of features $\\mathbf{z} \\in \\mathcal{R}^M$ explaining the *observation* variable $\\mathbf{x} \\in \\mathcal{R}^{P}$. Learning was performed by autoencoding $\\mathbf{x}$ through an *information bottleneck*. In this setting however, the quality of the representation is greatly impacted by the choice of the dimension $M$ of the bottleneck.\n",
        "\n",
        "The unobserved variable -- or *latent variable* -- can be modelled using a probabilistic framework, this allows us to build the model in a principled way, and overcomes many of the limitations of the basic autoencoder (e.g. choice of the dimension of the bottleneck). $\\mathbf{z}$ is chosen to represent the generative factors of $\\mathbf{x}$, we define a generative process \n",
        "\n",
        "$$\\mathbf{z} \\sim p_{\\theta}(\\mathbf{z}), \\  \\mathbf{x} \\sim p_{\\theta}(\\mathbf{x} | \\mathbf{z})$$ \n",
        "\n",
        "where the **prior** $p_{\\theta}(\\mathbf{z})$ is a chosen distribution (e.g. $\\mathcal{N}(0 , I)$) and the **observation model** $p_\\theta(\\mathbf{x} | \\mathbf{z})$ is a distribution depending on the variable $\\mathbf{z}$. Since we are interested in learning a model that explains well the data, we aim at maximizing the probability assigned to $\\mathbf{x}$. Therefore the optimal parameter $\\theta^\\star$ is given by\n",
        "\n",
        "$$\\theta^\\star := \\mathop{\\mathrm{argmax}}_\\theta p_\\theta (\\mathbf{x}) = \\int_\\mathbf{z} p_\\theta(\\mathbf{x}, \\mathbf{z}) d\\mathbf{z} = \\int_\\mathbf{z} p_\\theta(\\mathbf{x} | \\mathbf{z}) p_\\theta(\\mathbf{z}) d \\mathbf{z} \\ .$$\n",
        "\n",
        "## 2. Amortized Variational Inference: Estimatin the Likelihood \n",
        "\n",
        "### Intractability of the Likelihood\n",
        "\n",
        "In practice, $p_{\\theta}(\\mathbf{x})$ is **intractable**: marginalizing over $\\mathbf{z}$ is prohibitively expensive. A potential solution consists in using the *posterior distribution* which we can express using Bayes Rule:\n",
        "\n",
        "$$p_\\theta(\\mathbf{z} | \\mathbf{x}) =  \\frac{p_{\\boldsymbol{\\theta}}(\\mathbf{x}, \\mathbf{z})}{p_{\\boldsymbol{\\theta}}(\\mathbf{x})} \\ . $$\n",
        "\n",
        "However $p_\\theta(\\mathbf{x} | \\mathbf{z})$ is also intractable as it requires evaluating $p_\\theta (\\mathbf{x} )$.\n",
        "\n",
        "### Approximate Posterior\n",
        "\n",
        "**Variational Inference** (VI) overcomes the intractability of the *true posterior* $p_\\theta(\\mathbf{z} | \\mathbf{x})$ by introducing an **approximate posterior**\n",
        "\n",
        "$$q_\\phi(\\mathbf{z} | \\mathbf{x}) \\approx p_\\theta(\\mathbf{z} | \\mathbf{x}) \\ . $$ \n",
        "\n",
        "$q_\\phi(\\cdot| \\mathbf{x})$ is chosen among a *variational family* $\\mathcal{Q}$. A common choice for $\\mathcal{Q}$ is the Gaussian family, although it is possible to use other families (Categorical, Poisson, etc..). In the case of the Gaussian family, a common choice is the isotropic Gaussian (diagonal covariance)\n",
        "\n",
        "$$q_\\phi(\\mathbf{z} | \\mathbf{x}) := \\mathcal{N}( \\mathbf{z}\\ |\\ \\mu_\\phi(\\mathbf{x}) , \\sigma_\\phi(\\mathbf{x}) I )$$\n",
        "\n",
        "parameterized by neural networks $\\mu_\\phi(\\mathbf{x}) $ and $ \\sigma_\\phi(\\mathbf{x})$. Because the inference model $\\{ \\mu_\\phi, \\sigma_\\phi \\}$ is shared among all datapoints $\\{\\mathbf{x}_i, \\mathbf{y}_i\\}_{i=1, \\dots, N}$, we say that we use **amortized** Variational Inference. Non-amortized Variational Inference would consider one model $\\{ \\mu_\\phi, \\sigma_\\phi \\}$ for each datapoint $\\mathbf{x}$.\n",
        "\n",
        "The introduction of the approximate posterior allows to express the likelihood $p_\\theta (\\mathbf{x})$ as an expectation over $q_\\phi(\\mathbf{z} | \\mathbf{x})$:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "p_\\theta (\\mathbf{x})  = \\int_\\mathbf{z} p_\\theta(\\mathbf{x} ,\\mathbf{z}) d \\mathbf{z}\n",
        "= \\int_\\mathbf{z} \\frac{q_\\phi(\\mathbf{z} | \\mathbf{x})}{q_\\phi(\\mathbf{z} | \\mathbf{x})} p_\\theta(\\mathbf{x}, \\mathbf{z}) d \\mathbf{z}\n",
        "= \\mathbb{E}_{q_\\phi(\\mathbf{z} | \\mathbf{x})} \\left[\n",
        "\\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z} | \\mathbf{x})}\n",
        "\\right]\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "\n",
        "### Evidence Lower Bound (ELBO)\n",
        "\n",
        "Computing a Monte-Carlo estimate $\\mathbb{E}_{q_\\phi(\\mathbf{z} | \\mathbf{x})} \\left[ \\cdot \\right]$ of the ratio $\\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z} | \\mathbf{x})}$\n",
        "is computationally challenging due to its high variance and the numerically instability cause by the term $\\frac{1}{q_\\phi(\\mathbf{z} | \\mathbf{x})}$. Instead, we leverage well-behaved log-space computation thanks to [Jensen's Inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality), which gives the Evidence Lower Bound (ELBO) $\\mathcal{L} (\\mathbf{x})$:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\log p_\\theta (\\mathbf{x}) & = \\log \\mathbb{E}_{q_\\phi(\\mathbf{z} | \\mathbf{x})} \\left[ \n",
        "\\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z} | \\mathbf{x})} \n",
        "\\right]  \\geq  \\mathbb{E}_{q_\\phi(\\mathbf{z} | \\mathbf{x})} \\left[ \n",
        "\\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z} | \\mathbf{x})} \n",
        "\\right] =: \\mathcal{L} (\\mathbf{x})\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Is it possible to express the ELBO using [a KL divergence](https://en.wikipedia.org/wiki/Kullback–Leibler_divergence), which measure how two distributions differ from each other: \n",
        "\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{x}) := \\mathbb{E}_{q_\\phi(\\mathbf{z} | \\mathbf{x})} \\left[ \n",
        "\\log p_\\theta(\\mathbf{x} | \\mathbf{z}) - \\log q_\\phi(\\mathbf{z} | \\mathbf{x}) + \\log p_\\theta(\\mathbf{z}) \n",
        "\\right] =\n",
        "\\overbrace{\n",
        "\\mathbb{E}_{q_\\phi(\\mathbf{z} | \\mathbf{x})} \\left[ \\log p_\\theta(\\mathbf{x} | \\mathbf{z})\\right]\n",
        "}^{\\text{(a) Reconstruction Error}}\n",
        "- \n",
        "\\overbrace{\n",
        "\\mathcal{D}_{\\operatorname{KL}}\\left(q_\\phi(\\mathbf{z}|\\mathbf{x})\\ |\\ p(\\mathbf{z})\\right)\n",
        "}^{\\text{(b) Regularization}}\n",
        "$$\n",
        "\n",
        "Optimizing the ELBO results in a tradeoff between the terms (a) and (b). The term (a) mesaures the reconstruction quality while the term (b) enforces the posterior $q_\\phi(\\mathbf{z} | \\mathbf{x})$ to match the prior $p(\\mathbf{z})$. A stronger regularization (penalizing more the KL term b.), the more difficult it is to produce good reconstructions.\n",
        "\n",
        "\n",
        "## 3. Learning the Optimal Parameters: the Reparameterization Trick\n",
        "\n",
        "The ELBO is a lower bound to the log-likelihood,meaning that maximizing the ELBO results in maximizing the likelihood.\n",
        "Assuming that both the inference network $q_\\phi(\\mathbf{z} | \\mathbf{x})$ and\n",
        "the generative model $p_\\theta(\\mathbf{x} | \\mathbf{z})$ are implemented using network, we can apply backpropagation\n",
        "through all layers but not through the sampling operation $\\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$. In the next paragraph we will see how to optimize the parameters $\\theta$ and $\\phi$ w.r.t $\\mathcal{L}$.\n",
        "\n",
        "### Optimizing $\\theta$: Backropagation\n",
        "\n",
        "The ELBO is a lower bound to the marginal log-likelihood: $ \\mathcal{L} \\leq \\log p_\\theta (\\mathbf{x})$.\n",
        "Therefore we use stochastic gradient descent to minimize the negative ELBO. The gradient of the parameters $\\theta$ can be evaluated analytically\n",
        "using backpropagation (see the computational graph in the figure section *reparameterization trick*).\n",
        "\n",
        "###  Optimizing $\\phi$: Monte Carlo Gradient Estimation and the Reparameterization Trick\n",
        "\n",
        "**Estimating the gradient w.r.t $\\phi$ is challenging** due to the sampling operation $\\mathbf{z} \\sim q_{\\phi}(\\mathbf{z} | \\mathbf{x})$\n",
        "that requires integrating over $q_\\phi(\\mathbf{z}|\\mathbf{x})$. Here is an extensive review of the existing methods for\n",
        "[Monte Carlo Gradient Estimation in Machine Learning](https://arxiv.org/pdf/1906.10652.pdf).\n",
        "\n",
        "#### Naïve approach: Reinforce\n",
        "\n",
        "The usual Monte Carlo Gradient estimator for this type of problem is the Reinforce -- or the score-function -- gradient estimator.\n",
        "In a simplified setting of a loss function (score) of the general form $f_{\\theta}(\\mathbf{z},  \\mathbf{x})$, the following identify holds: $$\\nabla_{\\boldsymbol{\\phi}} \\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z} | \\mathbf{x})}[f_\\theta(\\mathbf{z}, \\mathbf{x})]=\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z} |  \\mathbf{x})}\\left[f_\\theta(\\mathbf{z},  \\mathbf{x}) \\nabla_{\\phi} \\log q_{\\boldsymbol{\\phi}}(\\mathbf{z} |  \\mathbf{x})\\right] \\text{  (Reinforce)}$$ . This gradient estimator suffers however from large variance, as a consequence learning will be inefficient.\n",
        "\n",
        "#### The Reparameterization Trick / Pathwise Derivatives\n",
        "\n",
        "<img src=\"https://github.com/DeepLearningDTU/02456-deep-learning-with-PyTorch/blob/master/7_Unsupervised/static/reparameterization.png?raw=1\" width=\"70%\" />\n",
        "\n",
        "The key component of the [the original Variational Autoencoder](https://arxiv.org/pdf/1312.6114.pdf) consists in using the Law of the Unconscious Statistician (LOTUS)\n",
        "to derive a **low-variance estimate of the gradient of $\\phi$**: this is the **Reparameterization Trick** (also known as pathwise derivatives). The trick consists in\n",
        "1. introducing a noise variable $\\epsilon$ with a fixed base distribution $p(\\epsilon)$\n",
        "2. introducing a deterministic and differentiable transformation $g_\\phi (\\epsilon, \\mathbf{x})$\n",
        "\n",
        "Such that $$\\mathbf{z} \\sim q_\\phi(\\mathbf{z}| \\mathbf{x}) \\text{ is equivalent to } \\mathbf{z} = g_\\phi(\\epsilon, \\mathbf{x}), \\epsilon \\sim p(\\epsilon) \\ .$$\n",
        "\n",
        "In this setting, a low-variance estimate of the gradient of the ELBO w.r.t $\\phi$ is given by:\n",
        "\n",
        "$$\\nabla_\\phi \\mathcal{L}(\\mathbf{x}) := \\nabla_\\phi \\mathbb{E}_{q_{\\phi}\\left(\\mathbf{z} \\mid \\mathbf{x}\\right)}[f_{\\theta, \\phi}(\\mathbf{z}, \\mathbf{x})]=\\mathbb{E}_{p(\\boldsymbol{\\epsilon})}\\left[ \\nabla_\\phi f_{\\theta, \\phi}\\left(\\tilde{\\mathbf{z}}, \\mathbf{x}\\right)\\right] \\text{  (Reparameterization)}$$\n",
        "\n",
        "with $\\tilde{\\mathbf{z}} := g_{\\boldsymbol{\\phi}}\\left(\\boldsymbol{\\epsilon},\\mathbf{x} \\right), \\epsilon \\sim p(\\epsilon) \\text{ and } f_{\\theta, \\phi}(\\mathbf{z}, \\mathbf{x}) := \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z}|\\mathbf{x})}$ .\n",
        "\n",
        "\n",
        "#### Choice of Reparameterization\n",
        "\n",
        "A common choice of parameterization is to choose $p(\\epsilon) = \\mathcal{N} (0, I)$ and parameterize a mean vector and a diagonal covariance matrix using neural networks $\\{\\mu_\\phi, \\sigma_\\phi\\}$:\n",
        "\n",
        "$$\\mathbf{z} = \\mu_\\phi(\\mathbf{x}) + \\sigma_\\phi(\\mathbf{x})  \\odot \\epsilon \\ \\text{   with   }  \\epsilon \\sim \\mathcal{N} (0, I) \\ . $$\n",
        "\n",
        "### Why a VAE learns a good approximate posterior $q_\\phi(\\mathbf{z} | \\mathbf{x}) \\approx p_\\theta(\\mathbf{z} | \\mathbf{x})$\n",
        "\n",
        "We have seen how to estimate the parameter $\\phi$ thanks to the reparameterization. Maximizing the ELBO will\n",
        "maximize its upper bound $\\log p_\\theta(\\mathbf{x})$. Yet, maximizing $\\log p_\\theta(\\mathbf{x})$ does not guarantee a good approximation\n",
        "$q_\\phi(\\mathbf{z} | \\mathbf{x}) \\approx p_\\theta(\\mathbf{z} | \\mathbf{x})$, and a poor approximation leads to a low-accuracy estimate of $\\log p_\\theta(\\mathbf{x})$.\n",
        "\n",
        "At the end of the note book, we show that the following identity holds:\n",
        "\n",
        "$$\\log p_\\theta(\\mathbf{x}) = \\mathcal{D}_{\\operatorname{KL}}(q_\\phi(\\mathbf{z} | \\mathbf{x}) | p_\\theta(\\mathbf{z}| \\mathbf{x})) + \\mathcal{L}(\\mathbf{x}) \\geq \\mathcal{L}(\\mathbf{x})$$\n",
        "\n",
        "Hence maximizing the ELBO also guarantees to push the approximate posterior $q_\\phi(\\mathbf{z}| \\mathbf{x})$ to be similar to the true posterior  $p_\\theta(\\mathbf{z}| \\mathbf{x})$ because $\\mathcal{D}_{\\operatorname{KL}}(q_\\phi(\\mathbf{z} | \\mathbf{x}) | p_\\theta(\\mathbf{z}| \\mathbf{x}))$ is minimized as $\\mathcal{L}$ is maximized.\n",
        "\n",
        "## 4. Interpreting VAEs as Autoencoders\n",
        "\n",
        "Similarly to the Autoencoder introduced in the previous notebook, the Variational Autoencoder admits an *encoder* model $g_\\phi(\\mathbf{x}, \\epsilon)$ parameterizing the posterior $q_\\phi(\\mathbf{z} | \\mathbf{x})$ and a *decoder* $h_\\theta(\\mathbf{z})$ parameterizing the observation model $p_\\theta(\\mathbf{x} | \\mathbf{z})$. Hence, a VAE is much similar to a classic autoencoder (as introduced in the notebook 7.1): instead of computing $\\mathbf{z} = h_\\phi(\\mathbf{x})$ (deterministic: autoencoder), a VAE exploits a stochastic model $\\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$ (see the top figure). \n",
        "\n",
        "When using the reparameterization-trick, this is equivalent to adding noise to the stochastic representation $\\mathbf{z} = \\mu_\\phi(\\mathbf{x}) + \\sigma_\\phi(\\mathbf{x})  \\odot \\epsilon$ with  $\\epsilon \\sim \\mathcal{N} (0, I)$.\n",
        "\n",
        "The ELBO introduces a tradeoff between reconstruction quality and the regularizer $\\mathcal{D}_{\\operatorname{KL}}\\left(q_\\phi(\\mathbf{z}|\\mathbf{x})\\ |\\ p(\\mathbf{z})\\right)$. It is possible to target different regularization strenghts using the following modified ELBO loss ([$\\beta-VAE$](https://openreview.net/references/pdf?id=Sy2fzU9gl)):\n",
        "\n",
        "$$\n",
        "\\mathcal{L}^{\\beta}(\\mathbf{x}) := \n",
        "\\overbrace{\n",
        "\\mathbb{E}_{q_\\phi(\\mathbf{z} | \\mathbf{x})} \\left[ \\log p_\\theta(\\mathbf{x} | \\mathbf{z})\\right]\n",
        "}^{\\text{(a) Reconstruction Error}}\n",
        "- \n",
        "\\beta\n",
        "\\cdot\n",
        "\\overbrace{\n",
        "\\mathcal{D}_{\\operatorname{KL}}\\left(q_\\phi(\\mathbf{z}|\\mathbf{x})\\ |\\ p(\\mathbf{z})\\right)\n",
        "}^{\\text{(b) Regularization}}\n",
        "$$\n",
        "\n",
        "\n",
        "## 5. Modelling Choices: How to design a VAE\n",
        "\n",
        "\n",
        "### Prior $p_{\\theta}(\\mathbf{z})$\n",
        "\n",
        "The prior distribution structures the latent space. Althougt it is common to choose a simple *variaitonal family* (Diagonal Gaussian or Categorical), more complex prior distributions can be used including complex factorizations (e.g. autoregressive: $p(\\mathbf{z}) = p_\\theta(\\mathbf{z_1}) \\prod_{l=1}^{L-1} p_{\\theta}(\\mathbf{z}_{l+1} | \\mathbf{z}_l) $ ). By default, the prior is commonly chosen to be a simple Gaussian distribution: $$p(\\mathbf{z}) := \\mathcal{N}(0, I)$$\n",
        "\n",
        "### Posterior $q_\\phi(\\mathbf{z}|\\mathbf{x})$\n",
        "\n",
        "The posterior is chosen to be of the same variational family as the prior. When using the reparameterization trick, the posterior must admit a reparameterization. A common choice fitting the Gaussian prior is a Diagonal Gaussian distribution: $$q_\\phi(\\mathbf{z} | \\mathbf{x}) := \\mathcal{N}( \\mathbf{z}\\ |\\ \\mu_\\phi(\\mathbf{x}) , \\sigma_\\phi(\\mathbf{x}) I )$$\n",
        "\n",
        "\n",
        "### Observation Model $p_\\theta(\\mathbf{x} | \\mathbf{z})$\n",
        "\n",
        "The choice of the observation model depends on the nature of the features, so for binary pixel values an appropiate choice of reconstruction distribution is the [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution), $$p_\\theta(\\mathbf{x}|\\mathbf{z}) = \\prod_i \\mathcal{B}(x_i | g_\\theta(\\mathbf{z})) = \\prod_i g_\\theta(\\mathbf{z})_i^{x_i} \\cdot (1-g_\\theta(\\mathbf{z})_i)^{1-x_i}$$ where $x_1, \\dots, x_D \\in \\{0,1\\}^{D}$ are the pixel values of the image $\\mathbf{x}$. $g_\\theta$ is the *decoder* of the VAE. $g_\\theta(z)_i$ is the probability of generating a 0 (black) or 1 (white) for the $i$-th pixel value. This is equivalent to modelling 784 imbalanced coin-tossing processes. This is only possible because we assume the pixel intensities to be i.i.d (Independent and Identically Distributed), which means $p(\\mathbf{x}) = \\prod_i p(x_i)$ , so no direct correlations between them needs to modelled, even though we still achieve an indirect conditional correlation through the latent variables, $\\mathbf{z}$.\n",
        "\n",
        "\n",
        "## 6. Evaluating a Variational Autoencoder\n",
        "\n",
        "### Assessing the Quality of the Samples\n",
        "\n",
        "The Variational Autoencoder defines a generative process $\\mathbf{z} \\sim p_{\\theta}(\\mathbf{z}), \\  \\mathbf{x} \\sim p_{\\theta}(\\mathbf{x} | \\mathbf{z})$. A *good* VAE should explain well the data $\\mathbf{x}$ and samples $\\mathbf{x} \\sim p_{\\theta}(\\mathbf{x} |\\mathbf{z}), \\mathbf{z} \\sim p_{\\theta}(\\mathbf{z})$ should be representive of the dataset.\n",
        "\n",
        "### Estimating the Likelihood\n",
        "\n",
        "A VAE defines a probabilistic model $p_\\theta(\\mathbf{x} | \\mathbf{z}) p(\\mathbf{z})$ and we are interested in maximizing the ability of the model to explain the dataset $\\mathcal{D} = \\{\\mathbf{x}_i\\}_{i=1, \\dots, N}$, hence we aim at obtaining the maximum probability $\\log p_\\theta(\\mathcal{D}) = \\sum_{i=1}^N \\log p_\\theta(\\mathbf{x}_i) =  \\sum_{i=1}^N \\log \\int_\\mathbf{z} p_\\theta(\\mathbf{x}_i, \\mathbf{z}) d\\mathbf{z} $. However, as discussed previously, the log-likelihood is intractable (marginalization over $\\mathbf{z}$), hence we rely on the Evidence Lower Bound (ELBO) as a proxy, or a tighter bound such as the importance weighted bound (see at the end of the notebook). \n",
        "\n",
        "**NB** It is common practice to report the average marginal log likelihood $\\log p_\\theta(\\mathcal{D}) / N$ and not $\\log p_\\theta(\\mathcal{D})$ directly:\n",
        "\n",
        "$$\\frac{1}{N} \\log p_\\theta(\\mathcal{D}) = \\frac{1}{N} \\sum_i \\log p_\\theta(\\mathbf{x}_i) \\geq \\frac{1}{N} \\sum_i \\operatorname{ELBO}(\\mathbf{x}_i) \\ . $$\n",
        "\n",
        "### Evaluation on Downstream Tasks\n",
        "\n",
        "As explained in the previous notebook, there is an interest in learning *compressed* representations $\\mathbf{z}$ of $\\mathbf{x}$ with the intent of solving downstream tasks such as classification. In this scenario, it is important to evaluate the VAE on the final task. For instance, learning a classifier using $\\mathbf{z}$ as features: $p(y | \\mathbf{z})$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRd7Dray5YI1"
      },
      "source": [
        "# Practice: Building and Training VAEs\n",
        "\n",
        "## Probabilistic Building Blocks\n",
        "\n",
        "First, we will implement modules representing probability distributions, which are essential in probabilistic machine learning. Here, we loosely follow the implementation from the `torch.distributions` package which provides modules for most of the commonly used distributions. \n",
        "\n",
        "### 1. Gaussian Distribution\n",
        "\n",
        "The Gaussian distribution is parameterized by two parameters:\n",
        "* the location parameter $\\mu$\n",
        "* the scale parameter $\\sigma \\geq 0$.\n",
        "\n",
        "\n",
        "**Exercise 1**: Implement a Gaussian distribution from the parameters $\\mu$ and $\\log \\sigma$ using the following code. The method `rsample()` should be compatible with the reparameterization trick."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ooNTJWBl5YI1"
      },
      "source": [
        "import math \n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.nn.functional import softplus\n",
        "from torch.distributions import Distribution\n",
        "\n",
        "\n",
        "class ReparameterizedDiagonalGaussian(Distribution):\n",
        "    \"\"\"\n",
        "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
        "    \"\"\"\n",
        "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
        "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
        "        self.mu = mu\n",
        "        self.sigma = log_sigma.exp()\n",
        "        \n",
        "    def sample_epsilon(self) -> Tensor:\n",
        "        \"\"\"`\\eps ~ N(0, I)`\"\"\"\n",
        "        return torch.empty_like(self.mu).normal_()\n",
        "        \n",
        "    def sample(self) -> Tensor:\n",
        "        \"\"\"sample `z ~ N(z | mu, sigma)` (without gradients)\"\"\"\n",
        "        with torch.no_grad():\n",
        "            return self.rsample()\n",
        "        \n",
        "    def rsample(self) -> Tensor:\n",
        "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
        "        raise NotImplementedError # <- your code\n",
        "        \n",
        "    def log_prob(self, z:Tensor) -> Tensor:\n",
        "        \"\"\"return the log probability: log `p(z)`\"\"\"\n",
        "        raise NotImplementedError # <- your code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "V3Kvqdrk5YI4"
      },
      "source": [
        "# test your implementation\n",
        "\n",
        "def test_normal_distribution():\n",
        "    \"\"\"a few safety checks for your implementation\"\"\"\n",
        "    N = 1000000\n",
        "    ones = torch.ones(torch.Size((N,)))\n",
        "    mu = 1.224 * ones\n",
        "    sigma = 0.689 * ones\n",
        "    dist = ReparameterizedDiagonalGaussian(mu, sigma.log())\n",
        "    z = dist.sample()\n",
        "    \n",
        "    # Expected value E[N(0, 1)] = 0\n",
        "    expected_z = z.mean()\n",
        "    diff = (expected_z - mu.mean())**2\n",
        "    assert diff < 1e-3, f\"diff = {diff}, expected_z = {expected_z}\"\n",
        "    \n",
        "    # Variance E[z**2 - E[z]**2]\n",
        "    var_z = (z**2 - expected_z**2).mean()\n",
        "    diff = (var_z - sigma.pow(2).mean())**2\n",
        "    assert diff < 1e-3, f\"diff = {diff}, var_z = {var_z}\"\n",
        "    \n",
        "    # log p(z)\n",
        "    from torch.distributions import Normal\n",
        "    base = Normal(loc=mu, scale=sigma)\n",
        "    diff = ((base.log_prob(z) - dist.log_prob(z))**2).mean()\n",
        "    assert diff < 1e-3, f\"diff = {diff}\"\n",
        "\n",
        "test_normal_distribution()   \n",
        "\n",
        "n_samples = 10000\n",
        "mu = torch.tensor([[0, 1]])\n",
        "sigma = torch.tensor([[0.5 , 3]])\n",
        "ones = torch.ones((1000,2))\n",
        "p = ReparameterizedDiagonalGaussian(mu=mu*ones, log_sigma=(sigma*ones).log())\n",
        "samples = p.sample()\n",
        "data = pd.DataFrame({\"x\": samples[:, 0], \"y\": samples[:, 1]})\n",
        "g = sns.jointplot(\n",
        "    data=data,\n",
        "    x=\"x\",y=\"y\",\n",
        "    kind=\"hex\",\n",
        "    ratio=10\n",
        ")\n",
        "plt.subplots_adjust(top=0.9)\n",
        "g.fig.suptitle(r\"$\\mathcal{N}(\\mathbf{y} \\mid \\mu, \\sigma)$\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "IBCQdjIW5YI6"
      },
      "source": [
        "### 2. Bernoulli Distribution\n",
        "\n",
        "The Bernoulli distribution is a good fit when modelling binary outcomes (e.g. coin flipping). Given a binary random variable $X$ with outcomes $y \\in \\{0, 1\\}$, the probability density of the Bernoulli model with a parameter $\\theta$ is defined as\n",
        "$$\\mathcal{B}( y \\mid \\theta) = \\theta^{y} (1-\\theta)^{1-y},\\quad \\theta \\in [0,1]$$\n",
        "\n",
        "**Exercise 1**: Import the `Bernoulli` from the [torch.distributions](https://pytorch.org/docs/stable/distributions.html) package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_g5-lCco5YI6"
      },
      "source": [
        "# <- your code\n",
        "\n",
        "p = Bernoulli(logits=torch.zeros((1000,)))\n",
        "plt.figure(figsize=(12, 3))\n",
        "sns.distplot(p.sample())\n",
        "plt.title(r\"$\\mathcal{B}(\\mathbf{y} \\mid \\mathbf{\\theta})$\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "nQKmVckz5YI8"
      },
      "source": [
        "## Dataset: MNIST\n",
        "\n",
        "First let us load the MNIST dataset and plot a few examples. We only load a limited amount of classes, controlled through the `classes` variable, to speed up training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "_Vq_bddD5YI9"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.transforms import ToTensor\n",
        "from functools import reduce\n",
        "\n",
        "# Flatten the images into a vector\n",
        "flatten = lambda x: ToTensor()(x).view(28**2)\n",
        "\n",
        "# Define the train and test sets\n",
        "dset_train = MNIST(\"./\", train=True,  transform=flatten, download=True)\n",
        "dset_test  = MNIST(\"./\", train=False, transform=flatten)\n",
        "\n",
        "# The digit classes to use\n",
        "classes = [3, 7]\n",
        "\n",
        "def stratified_sampler(labels):\n",
        "    \"\"\"Sampler that only picks datapoints corresponding to the specified classes\"\"\"\n",
        "    (indices,) = np.where(reduce(lambda x, y: x | y, [labels.numpy() == i for i in classes]))\n",
        "    indices = torch.from_numpy(indices)\n",
        "    return SubsetRandomSampler(indices)\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "eval_batch_size = 100\n",
        "# The loaders perform the actual work\n",
        "train_loader = DataLoader(dset_train, batch_size=batch_size,\n",
        "                          sampler=stratified_sampler(dset_train.train_labels))\n",
        "test_loader  = DataLoader(dset_test, batch_size=eval_batch_size, \n",
        "                          sampler=stratified_sampler(dset_test.test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJaNuidz5YI_"
      },
      "source": [
        "#plot a few MNIST examples\n",
        "f, axarr = plt.subplots(4, 16, figsize=(16, 4))\n",
        "\n",
        "# Load a batch of images into memory\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "for i, ax in enumerate(axarr.flat):\n",
        "    ax.imshow(images[i].view(28, 28), cmap=\"binary_r\")\n",
        "    ax.axis('off')\n",
        "    \n",
        "plt.suptitle('MNIST handwritten digits')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "H9zo5pvZ5YJC"
      },
      "source": [
        "## Building the model\n",
        "When defining the model the latent layer must act as a bottleneck of information, so that we ensure that we find a strong internal representation. We initialize the VAE with 1 hidden layer in the encoder and decoder using relu units as non-linearity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "l-RvByaM5YJC"
      },
      "source": [
        "class VariationalAutoencoder(nn.Module):\n",
        "    \"\"\"A Variational Autoencoder with\n",
        "    * a Bernoulli observation model `p_\\theta(x | z) = B(x | g_\\theta(z))`\n",
        "    * a Gaussian prior `p(z) = N(z | 0, I)`\n",
        "    * a Gaussian posterior `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x))`\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_shape:torch.Size, latent_features:int) -> None:\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        \n",
        "        self.input_shape = input_shape\n",
        "        self.latent_features = latent_features\n",
        "        self.observation_features = np.prod(input_shape)\n",
        "        \n",
        "\n",
        "        # Inference Network\n",
        "        # Encode the observation `x` into the parameters of the posterior distribution\n",
        "        # `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x)), \\mu(x),\\log\\sigma(x) = h_\\phi(x)`\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_features=self.observation_features, out_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=256, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            # A Gaussian is fully characterised by its mean \\mu and variance \\sigma**2\n",
        "            nn.Linear(in_features=128, out_features=2*latent_features) # <- note the 2*latent_features\n",
        "        )\n",
        "        \n",
        "        # Generative Model\n",
        "        # Decode the latent sample `z` into the parameters of the observation model\n",
        "        # `p_\\theta(x | z) = \\prod_i B(x_i | g_\\theta(x))`\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(in_features=latent_features, out_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=128, out_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=256, out_features=self.observation_features)\n",
        "        )\n",
        "        \n",
        "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
        "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
        "        \n",
        "    def posterior(self, x:Tensor) -> Distribution:\n",
        "        \"\"\"return the distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
        "        \n",
        "        # compute the parameters of the posterior\n",
        "        h_x = self.encoder(x)\n",
        "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
        "        \n",
        "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
        "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
        "    \n",
        "    def prior(self, batch_size:int=1)-> Distribution:\n",
        "        \"\"\"return the distribution `p(z)`\"\"\"\n",
        "        prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:])\n",
        "        mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
        "        \n",
        "        # return the distribution `p(z)`\n",
        "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
        "    \n",
        "    def observation_model(self, z:Tensor) -> Distribution:\n",
        "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
        "        px_logits = self.decoder(z)\n",
        "        px_logits = px_logits.view(-1, *self.input_shape) # reshape the output\n",
        "        return Bernoulli(logits=px_logits)\n",
        "        \n",
        "\n",
        "    def forward(self, x) -> Dict[str, Any]:\n",
        "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
        "        \n",
        "        # flatten the input\n",
        "        x = x.view(x.size(0), -1)\n",
        "        \n",
        "        # define the posterior q(z|x) / encode x into q(z|x)\n",
        "        qz = self.posterior(x)\n",
        "        \n",
        "        # define the prior p(z)\n",
        "        pz = self.prior(batch_size=x.size(0))\n",
        "        \n",
        "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
        "        z = qz.rsample()\n",
        "        \n",
        "        # define the observation model p(x|z) = B(x | g(z))\n",
        "        px = self.observation_model(z)\n",
        "        \n",
        "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}\n",
        "    \n",
        "    \n",
        "    def sample_from_prior(self, batch_size:int=100):\n",
        "        \"\"\"sample z~p(z) and return p(x|z)\"\"\"\n",
        "        \n",
        "        # degine the prior p(z)\n",
        "        pz = self.prior(batch_size=batch_size)\n",
        "        \n",
        "        # sample the prior \n",
        "        z = pz.rsample()\n",
        "        \n",
        "        # define the observation model p(x|z) = B(x | g(z))\n",
        "        px = self.observation_model(z)\n",
        "        \n",
        "        return {'px': px, 'pz': pz, 'z': z}\n",
        "\n",
        "\n",
        "latent_features = 2\n",
        "vae = VariationalAutoencoder(images[0].shape, latent_features)\n",
        "print(vae)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ymFbwY365YJF"
      },
      "source": [
        "## Implement a module for Variational Inference\n",
        "\n",
        "**Exercise 1**: implement `elbo` ($\\mathcal{L}$) and `beta_elbo` ($\\mathcal{L}^\\beta$)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pETwtuoJ5YJF"
      },
      "source": [
        "def reduce(x:Tensor) -> Tensor:\n",
        "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
        "    return x.view(x.size(0), -1).sum(dim=1)\n",
        "\n",
        "class VariationalInference(nn.Module):\n",
        "    def __init__(self, beta:float=1.):\n",
        "        super().__init__()\n",
        "        self.beta = beta\n",
        "        \n",
        "    def forward(self, model:nn.Module, x:Tensor) -> Tuple[Tensor, Dict]:\n",
        "        \n",
        "        # forward pass through the model\n",
        "        outputs = model(x)\n",
        "        \n",
        "        # unpack outputs\n",
        "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
        "        \n",
        "        # evaluate log probabilities\n",
        "        log_px = reduce(px.log_prob(x))\n",
        "        log_pz = reduce(pz.log_prob(z))\n",
        "        log_qz = reduce(qz.log_prob(z))\n",
        "        \n",
        "        # compute the ELBO with and without the beta parameter: \n",
        "        # `L^\\beta = E_q [ log p(x|z) - \\beta * D_KL(q(z|x) | p(z))`\n",
        "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
        "        kl = log_qz - log_pz\n",
        "        elbo = # <- your code here\n",
        "        beta_elbo = # <- your code here\n",
        "        \n",
        "        # loss\n",
        "        loss = -beta_elbo.mean()\n",
        "        \n",
        "        # prepare the output\n",
        "        with torch.no_grad():\n",
        "            diagnostics = {'elbo': elbo, 'log_px':log_px, 'kl': kl}\n",
        "            \n",
        "        return loss, diagnostics, outputs\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "kLyA2Gsl5YJH"
      },
      "source": [
        "vi = VariationalInference(beta=1.0)\n",
        "loss, diagnostics, outputs = vi(vae, images)\n",
        "print(f\"{'loss':6} | mean = {loss:10.3f}, shape: {list(loss.shape)}\")\n",
        "for key, tensor in diagnostics.items():\n",
        "    print(f\"{key:6} | mean = {tensor.mean():10.3f}, shape: {list(tensor.shape)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "1mztY9kW5YJJ"
      },
      "source": [
        "## Training and Evaluation\n",
        "\n",
        "### Initialize the model, evaluator and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "5UeTSeEq5YJJ"
      },
      "source": [
        "from collections import defaultdict\n",
        "# define the models, evaluator and optimizer\n",
        "\n",
        "# VAE\n",
        "latent_features = 2\n",
        "vae = VariationalAutoencoder(images[0].shape, latent_features)\n",
        "\n",
        "# Evaluator: Variational Inference\n",
        "beta = 1\n",
        "vi = VariationalInference(beta=beta)\n",
        "\n",
        "# The Adam optimizer works really well with VAEs.\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
        "\n",
        "# define dictionary to store the training curves\n",
        "training_data = defaultdict(list)\n",
        "validation_data = defaultdict(list)\n",
        "\n",
        "epoch = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "pCzDQm2W5YJL"
      },
      "source": [
        "### Training Loop\n",
        "\n",
        "**plotting guide**:\n",
        "\n",
        "* 1st row: Reproducing the figure from the begining of the Notebook.\n",
        "    * (Left) Data. \n",
        "    * (Middle) Latent space: the large gray disk reprensents the prior (radius = $2\\sigma$), each point represents a latent sample $\\mathbf{z}$. The smaller ellipses represent the distributions $q_\\phi(\\mathbf{z} | \\mathbf{x})$  (radius = $2\\sigma$). When using $\\geq 2$ latent features, dimensionality reduction is applied using t-SNE and only samples $\\mathbf{z} \\sim q_\\phi(\\mathbf{z} | \\mathbf{x})$ are displayed. \n",
        "    * (Right) samples from $p_\\theta(\\mathbf{x} | \\mathbf{z})$.\n",
        "\n",
        "* 2nd row: Training curves\n",
        "\n",
        "* 2rd row: Latent samples. \n",
        "    * (Left) Prior samples $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | \\mathbf{z}), \\mathbf{z} \\sim p(\\mathbf{z})$ \n",
        "    * (Middle) Latent Interpolations. For each row: $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | t \\cdot \\mathbf{z}_1 + (1-t) \\cdot \\mathbf{z}_2), \\mathbf{z}_1, \\mathbf{z}_2 \\sim p(\\mathbf{z}), t=0 \\dots 1$. \n",
        "    * (Right): Sampling $\\mathbf{z}$ from a grid [-3:3, -3:3] $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} | \\mathbf{z}), \\mathbf{z} \\sim \\operatorname{grid}(-3:3, -3:3)$ (only available for 2d latent space).\n",
        "\n",
        "**NOTE** this will take a while on CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "BTVgHNhg5YJM"
      },
      "source": [
        "num_epochs = 100\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\">> Using device: {device}\")\n",
        "\n",
        "# move the model to the device\n",
        "vae = vae.to(device)\n",
        "\n",
        "# training..\n",
        "while epoch < num_epochs:\n",
        "    epoch+= 1\n",
        "    training_epoch_data = defaultdict(list)\n",
        "    vae.train()\n",
        "    \n",
        "    # Go through each batch in the training dataset using the loader\n",
        "    # Note that y is not necessarily known as it is here\n",
        "    for x, y in train_loader:\n",
        "        x = x.to(device)\n",
        "        \n",
        "        # perform a forward pass through the model and compute the ELBO\n",
        "        loss, diagnostics, outputs = vi(vae, x)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # gather data for the current bach\n",
        "        for k, v in diagnostics.items():\n",
        "            training_epoch_data[k] += [v.mean().item()]\n",
        "            \n",
        "\n",
        "    # gather data for the full epoch\n",
        "    for k, v in training_epoch_data.items():\n",
        "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
        "\n",
        "    # Evaluate on a single batch, do not propagate gradients\n",
        "    with torch.no_grad():\n",
        "        vae.eval()\n",
        "        \n",
        "        # Just load a single batch from the test loader\n",
        "        x, y = next(iter(test_loader))\n",
        "        x = x.to(device)\n",
        "        \n",
        "        # perform a forward pass through the model and compute the ELBO\n",
        "        loss, diagnostics, outputs = vi(vae, x)\n",
        "        \n",
        "        # gather data for the validation step\n",
        "        for k, v in diagnostics.items():\n",
        "            validation_data[k] += [v.mean().item()]\n",
        "    \n",
        "    # Reproduce the figure from the begining of the notebook, plot the training curves and show latent samples\n",
        "    make_vae_plots(vae, x, y, outputs, training_data, validation_data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}